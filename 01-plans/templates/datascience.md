# IRL Plan: Data Science Project

## Setup
Create on first run:
- plans/
- plans/_quarto.yml   # Configure: render: ["*.qmd"] to exclude .md files
- data/raw/           # Original, immutable data files
- data/processed/     # Cleaned, transformed data
- data/interim/       # Intermediate transformations
- notebooks/          # Exploratory Jupyter/Quarto notebooks
- src/                # Reusable code (functions, pipelines)
- models/             # Trained models and artifacts
- outputs/figures/    # Generated visualizations
- outputs/tables/     # Generated tables
- outputs/reports/    # Final reports and presentations
- logs/

## Before Each Run
- git commit before edits
- do not edit this file

## Project Metadata
- Project name: [Title]
- Principal investigator: [Name]
- Analyst: [Name]
- Start date: [YYYY-MM-DD]
- Status: [planning/exploratory/confirmatory/complete]

## Research Context

### Background
- [Brief context and motivation for the analysis]
- [What is already known in this domain]

### Significance
- [Why this analysis matters]
- [Potential impact of findings]

### Objectives
- Primary: [Main research question or goal]
- Secondary: [Supporting questions, if any]

### Hypotheses (if confirmatory)
- H1: [Pre-specified hypothesis]
- H0: [Null hypothesis]

## Data Specification

### Data Sources
| Source | Description | Format | Location |
|--------|-------------|--------|----------|
| [Name] | [What it contains] | [CSV/Parquet/etc.] | data/raw/ |

### Data Dictionary
- Document variables in: data/data-dictionary.md
- Include: variable name, type, units, valid range, missing codes

### Inclusion/Exclusion Criteria
- Include: [criteria for including observations]
- Exclude: [criteria for excluding observations]
- Document exclusions with counts in logs/

## Approach
- Reproducibility first: all analyses scripted, no manual steps
- Raw data is read-only: never modify files in data/raw/
- Notebooks for exploration, scripts for production
- Version data transformations: document each processing step
- Separate exploratory (EDA) from confirmatory analyses
- Use seeds for all random operations
- Environment management: requirements.txt, renv.lock, or environment.yml

## Analysis Plan

### Phase 1: Data Ingestion and Validation
- Load and verify data integrity (checksums, row counts)
- Validate against data dictionary
- Check for unexpected values, out-of-range data
- Document any data quality issues
- Output: data/processed/validated-data.[format]

### Phase 2: Exploratory Data Analysis (EDA)
- Univariate summaries (distributions, missingness)
- Bivariate relationships (correlations, cross-tabs)
- Identify outliers and anomalies
- Generate hypotheses (document, don't test formally)
- Output: notebooks/01-eda.qmd, outputs/figures/eda-*

### Phase 3: Data Preprocessing
- Missing data handling: [strategy - MCAR/MAR/MNAR approach]
- Outlier treatment: [strategy - winsorize/remove/flag]
- Transformations: [log, standardize, normalize, etc.]
- Output: data/processed/analysis-ready.[format]

### Phase 4: Feature Engineering
- Derived variables: [list planned features]
- Encoding: [categorical handling strategy]
- Feature selection: [method if applicable]
- Document rationale for each engineered feature
- Output: data/processed/features.[format]

### Phase 5: Modeling (if applicable)
- Algorithm(s): [specify methods]
- Validation strategy: [holdout/k-fold/time-series split]
- Hyperparameter tuning: [approach]
- Evaluation metrics: [primary and secondary]
- Output: models/, outputs/tables/model-performance.*

### Phase 6: Results and Reporting
- Primary analysis results
- Sensitivity analyses
- Visualizations for key findings
- Final report compilation
- Output: outputs/reports/final-report.qmd

## Tasks

### Task 1: Environment Setup
- Initialize project structure
- Create requirements.txt or environment.yml
- Set up version control for data (DVC, git-lfs, or checksums)
- Document compute environment

### Task 2: Data Ingestion
- Source: [specify data source]
- Load data into data/raw/
- Create data/data-dictionary.md
- Generate checksums for raw data files
- Initial row/column counts

### Task 3: Data Validation
- Validate against data dictionary
- Check data types and ranges
- Assess missingness patterns
- Document quality issues in logs/data-quality.md

### Task 4: Exploratory Analysis
- Generate summary statistics
- Create distribution plots
- Correlation analysis
- Document observations in notebooks/01-eda.qmd

### Task 5: Preprocessing Pipeline
- Implement cleaning steps in src/preprocess.py (or .R)
- Handle missing data
- Apply transformations
- Save processed data with versioning

### Task 6: [Analysis-Specific Task]
- [Describe specific analysis]
- Method: [statistical/ML approach]
- Output: [expected deliverable]

### Task 7: Results Reporting
- Generate publication-quality figures
- Create summary tables
- Draft results narrative
- Compile into outputs/reports/

## Reproducibility Checklist
- [ ] Random seeds set for all stochastic operations
- [ ] Environment frozen (requirements.txt, renv.lock)
- [ ] Raw data checksums recorded
- [ ] All manual steps documented or automated
- [ ] Code runs end-to-end from raw data to outputs
- [ ] Results match when re-run

## Quality Control
- [ ] Code reviewed by second analyst
- [ ] Results sanity-checked against expectations
- [ ] Edge cases and subgroups examined
- [ ] Sensitivity analyses performed

## After Each Run
- Update logs/activity.md with:
  - Analysis steps completed
  - Key findings or observations
  - Decisions made and rationale
  - Issues encountered
- Verify outputs generated correctly
- Check for unintended data leakage
- git commit changes with descriptive message
- If exploratory: note hypotheses generated (not tested)
- If confirmatory: compare to pre-specified analysis plan
